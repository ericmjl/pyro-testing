{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pyro\n",
    "from pyro.distributions import Normal\n",
    "from pyro.infer import SVI\n",
    "from pyro.optim import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 100  # size of toy data\n",
    "p = 1    # number of features\n",
    "\n",
    "def build_linear_dataset(N, noise_std=0.1):\n",
    "    X = np.linspace(-6, 6, num=N)\n",
    "    y = 3 * X + 1 + np.random.normal(0, noise_std, size=N)\n",
    "    X, y = X.reshape((N, 1)), y.reshape((N, 1))\n",
    "    X, y = Variable(torch.Tensor(X)), Variable(torch.Tensor(y))\n",
    "    return torch.cat((X, y), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(p, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "regression_model = RegressionModel(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "optim = torch.optim.Adam(regression_model.parameters(), lr=0.01)\n",
    "num_iterations = 2000\n",
    "\n",
    "def main():\n",
    "    data = build_linear_dataset(N, p)\n",
    "    x_data = data[:, :-1]\n",
    "    y_data = data[:, -1]\n",
    "    for j in range(num_iterations):\n",
    "        # run the model forward on the data\n",
    "        y_pred = regression_model(x_data)\n",
    "        # calculate the mse loss\n",
    "        loss = loss_fn(y_pred, y_data)\n",
    "        # initialize gradients to zero\n",
    "        optim.zero_grad()\n",
    "        # backpropagate\n",
    "        loss.backward()\n",
    "        # take a gradient step\n",
    "        optim.step()\n",
    "        if (j + 1) % 50 == 0:\n",
    "            print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss.data[0]))\n",
    "    # Inspect learned parameters\n",
    "    print(\"Learned parameters:\")\n",
    "    for name, param in regression_model.named_parameters():\n",
    "        print(\"%s: %.3f\" % (name, param.data.numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0050] loss: 4002.3042\n",
      "[iteration 0100] loss: 2344.6309\n",
      "[iteration 0150] loss: 1298.9349\n",
      "[iteration 0200] loss: 688.2485\n",
      "[iteration 0250] loss: 360.9415\n",
      "[iteration 0300] loss: 201.0548\n",
      "[iteration 0350] loss: 130.1795\n",
      "[iteration 0400] loss: 101.7277\n",
      "[iteration 0450] loss: 91.3897\n",
      "[iteration 0500] loss: 87.9892\n",
      "[iteration 0550] loss: 86.9765\n",
      "[iteration 0600] loss: 86.7035\n",
      "[iteration 0650] loss: 86.6369\n",
      "[iteration 0700] loss: 86.6222\n",
      "[iteration 0750] loss: 86.6193\n",
      "[iteration 0800] loss: 86.6188\n",
      "[iteration 0850] loss: 86.6187\n",
      "[iteration 0900] loss: 86.6187\n",
      "[iteration 0950] loss: 86.6187\n",
      "[iteration 1000] loss: 86.6187\n",
      "[iteration 1050] loss: 86.6187\n",
      "[iteration 1100] loss: 86.6187\n",
      "[iteration 1150] loss: 86.6187\n",
      "[iteration 1200] loss: 86.6187\n",
      "[iteration 1250] loss: 86.6186\n",
      "[iteration 1300] loss: 86.6186\n",
      "[iteration 1350] loss: 86.6186\n",
      "[iteration 1400] loss: 86.6186\n",
      "[iteration 1450] loss: 86.6186\n",
      "[iteration 1500] loss: 86.6186\n",
      "[iteration 1550] loss: 86.6187\n",
      "[iteration 1600] loss: 86.6187\n",
      "[iteration 1650] loss: 86.6187\n",
      "[iteration 1700] loss: 86.6187\n",
      "[iteration 1750] loss: 86.6187\n",
      "[iteration 1800] loss: 86.6187\n",
      "[iteration 1850] loss: 86.6187\n",
      "[iteration 1900] loss: 86.6187\n",
      "[iteration 1950] loss: 86.6187\n",
      "[iteration 2000] loss: 86.6187\n",
      "Learned parameters:\n",
      "linear.weight: 2.994\n",
      "linear.bias: 1.062\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu = Variable(torch.zeros(1, 1))\n",
    "sigma = Variable(torch.ones(1, 1))\n",
    "# define a unit normal prior\n",
    "prior = Normal(mu, sigma)\n",
    "# overload the parameters in the regression nn with samples from the prior\n",
    "lifted_module = pyro.random_module(\"regression_module\", regression_model, prior)\n",
    "# sample a nn from the prior\n",
    "sampled_nn = lifted_module()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(data):\n",
    "    # Create unit normal priors over the parameters\n",
    "    x_data = data[:, :-1]\n",
    "    y_data = data[:, -1]\n",
    "    mu, sigma = Variable(torch.zeros(p, 1)), Variable(10 * torch.ones(p, 1))\n",
    "    bias_mu, bias_sigma = Variable(torch.zeros(1)), Variable(10 * torch.ones(1))\n",
    "    w_prior, b_prior = Normal(mu, sigma), Normal(bias_mu, bias_sigma)\n",
    "    priors = {'linear.weight': w_prior, 'linear.bias': b_prior}\n",
    "    \n",
    "    # lift module parameters to random variables\n",
    "    lifted_module = pyro.random_module(\"module\", regression_model, priors)\n",
    "    \n",
    "    # sample a nn (which also samples w and b)\n",
    "    lifted_nn = lifted_module()\n",
    "    \n",
    "    # run the nn forward\n",
    "    latent = lifted_nn(x_data).squeeze()\n",
    "    \n",
    "    # condition on the observed data\n",
    "    pyro.observe(\"obs\", Normal(latent, Variable(0.1 * torch.ones(data.size(0)))),\n",
    "                 y_data.squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "softplus = torch.nn.Softplus()\n",
    "\n",
    "def guide(data):\n",
    "    # define our variational parameters\n",
    "    w_mu = Variable(torch.randn(p, 1), requires_grad=True)\n",
    "    \n",
    "    # note that we initialize our sigmas to be pretty narrow\n",
    "    w_log_sig = Variable(-3.0 * torch.ones(p, 1) + 0.05 * torch.randn(p, 1),\n",
    "                         requires_grad=True)\n",
    "    b_mu = Variable(torch.randn(1), requires_grad=True)\n",
    "    b_log_sig = Variable(-3.0 * torch.ones(1) + 0.05 * torch.randn(1),\n",
    "                         requires_grad=True)\n",
    "    \n",
    "    # register learnable params in the param store\n",
    "    mw_param = pyro.param(\"guide_mean_weight\", w_mu)\n",
    "    sw_param = softplus(pyro.param(\"guide_log_sigma_weight\", w_log_sig))\n",
    "    mb_param = pyro.param(\"guide_mean_bias\", b_mu)\n",
    "    sb_param = softplus(pyro.param(\"guide_log_sigma_bias\", b_log_sig))\n",
    "    \n",
    "    # guide distributions for w and b\n",
    "    w_dist, b_dist = Normal(mw_param, sw_param), Normal(mb_param, sb_param)\n",
    "    dists = {'linear.weight': w_dist, 'linear.bias': b_dist}\n",
    "    \n",
    "    # overload the parameters in the module with random samples\n",
    "    # from the guide distributions\n",
    "    lifted_module = pyro.random_module(\"module\", regression_model, dists)\n",
    "    \n",
    "    # sample a nn (which also samples w and b)\n",
    "    return lifted_module()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optim = Adam({\"lr\": 0.01})\n",
    "svi = SVI(model, guide, optim, loss=\"ELBO\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    pyro.clear_param_store()\n",
    "    data = build_linear_dataset(N, p)\n",
    "    for j in range(num_iterations):\n",
    "        # calculate the loss and take a gradient step\n",
    "        loss = svi.step(data)\n",
    "        if j % 100 == 0:\n",
    "            print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss / float(N)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0001] loss: 3270.7712\n",
      "[iteration 0101] loss: 1088.8938\n",
      "[iteration 0201] loss: 330.5920\n",
      "[iteration 0301] loss: 92.8331\n",
      "[iteration 0401] loss: 53.6706\n",
      "[iteration 0501] loss: 47.4898\n",
      "[iteration 0601] loss: 48.9851\n",
      "[iteration 0701] loss: 45.7022\n",
      "[iteration 0801] loss: 45.8497\n",
      "[iteration 0901] loss: 47.1203\n",
      "[iteration 1001] loss: 46.2104\n",
      "[iteration 1101] loss: 46.1773\n",
      "[iteration 1201] loss: 46.7581\n",
      "[iteration 1301] loss: 45.8975\n",
      "[iteration 1401] loss: 46.5042\n",
      "[iteration 1501] loss: 46.0531\n",
      "[iteration 1601] loss: 45.8823\n",
      "[iteration 1701] loss: 45.6859\n",
      "[iteration 1801] loss: 45.6970\n",
      "[iteration 1901] loss: 46.1963\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[guide_mean_weight]: 3.006\n",
      "[guide_log_sigma_weight]: -3.902\n",
      "[guide_mean_bias]: 1.033\n",
      "[guide_log_sigma_bias]: -3.685\n"
     ]
    }
   ],
   "source": [
    "for name in pyro.get_param_store().get_all_param_names():\n",
    "    print(\"[%s]: %.3f\" % (name, pyro.param(name).data.numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyro",
   "language": "python",
   "name": "pyro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
